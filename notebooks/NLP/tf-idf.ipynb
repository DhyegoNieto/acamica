{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del lenguaje natural con Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf vectorizador\n",
    "\n",
    "Tf-idf se modela con la funcion `TfidfVectorizer`\n",
    "\n",
    "Esta funcion puede realizar en un solo paso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Limpieza\n",
    "    1. Minusculas\n",
    "    1. Eliminar acentos\n",
    "    1. Stop words\n",
    "1. Tokenizar por\n",
    "    1. palabras\n",
    "    1. caracteres\n",
    "    1. expresion regular\n",
    "1. Matriz Tf-Idf\n",
    "    1. con `fit-transform`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar el vectorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametros de TfidVectorizer\n",
    "\n",
    "(seleccion de los mas utilizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**input** : string {'filename', 'file', 'content'}\n",
    "\n",
    "**encoding** : string, 'utf-8' by default.\n",
    "\n",
    "**strip_accents** : {'ascii', 'unicode', None}\n",
    "    Remove accents and perform other character normalization during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have an direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "**lowercase** : boolean, default True\n",
    "\n",
    "**analyzer** : string, {'word', 'char'} or callable\n",
    "    Whether the feature should be made of word or character n-grams.\n",
    "\n",
    "**stop_words** : string {'english'}, list, or None (default)\n",
    "    'english' is currently the only supported string value. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "\n",
    "**token_pattern** : string\n",
    "    Regular expression denoting what constitutes a \"token\", only used if ``analyzer == 'word'``\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista completa de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m[\u001b[0m\u001b[1;34m\"input='content'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding='utf-8'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"decode_error='strict'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'strip_accents=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lowercase=True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'preprocessor=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tokenizer=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"analyzer='word'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stop_words=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ngram_range=(1, 1)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_df=1.0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min_df=1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_features=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary=None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'binary=False'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype=<class 'numpy.float64'>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"norm='l2'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'use_idf=True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'smooth_idf=True'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sublinear_tf=False'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
       "\n",
       "Equivalent to CountVectorizer followed by TfidfTransformer.\n",
       "\n",
       "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "input : string {'filename', 'file', 'content'}\n",
       "    If 'filename', the sequence passed as an argument to fit is\n",
       "    expected to be a list of filenames that need reading to fetch\n",
       "    the raw content to analyze.\n",
       "\n",
       "    If 'file', the sequence items must have a 'read' method (file-like\n",
       "    object) that is called to fetch the bytes in memory.\n",
       "\n",
       "    Otherwise the input is expected to be the sequence strings or\n",
       "    bytes items are expected to be analyzed directly.\n",
       "\n",
       "encoding : string, 'utf-8' by default.\n",
       "    If bytes or files are given to analyze, this encoding is used to\n",
       "    decode.\n",
       "\n",
       "decode_error : {'strict', 'ignore', 'replace'}\n",
       "    Instruction on what to do if a byte sequence is given to analyze that\n",
       "    contains characters not of the given `encoding`. By default, it is\n",
       "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
       "    values are 'ignore' and 'replace'.\n",
       "\n",
       "strip_accents : {'ascii', 'unicode', None}\n",
       "    Remove accents and perform other character normalization\n",
       "    during the preprocessing step.\n",
       "    'ascii' is a fast method that only works on characters that have\n",
       "    an direct ASCII mapping.\n",
       "    'unicode' is a slightly slower method that works on any characters.\n",
       "    None (default) does nothing.\n",
       "\n",
       "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
       "    :func:`unicodedata.normalize`.\n",
       "\n",
       "lowercase : boolean, default True\n",
       "    Convert all characters to lowercase before tokenizing.\n",
       "\n",
       "preprocessor : callable or None (default)\n",
       "    Override the preprocessing (string transformation) stage while\n",
       "    preserving the tokenizing and n-grams generation steps.\n",
       "\n",
       "tokenizer : callable or None (default)\n",
       "    Override the string tokenization step while preserving the\n",
       "    preprocessing and n-grams generation steps.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "analyzer : string, {'word', 'char'} or callable\n",
       "    Whether the feature should be made of word or character n-grams.\n",
       "\n",
       "    If a callable is passed it is used to extract the sequence of features\n",
       "    out of the raw, unprocessed input.\n",
       "\n",
       "stop_words : string {'english'}, list, or None (default)\n",
       "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
       "    list is returned. 'english' is currently the only supported string\n",
       "    value.\n",
       "    There are several known issues with 'english' and you should\n",
       "    consider an alternative (see :ref:`stop_words`).\n",
       "\n",
       "    If a list, that list is assumed to contain stop words, all of which\n",
       "    will be removed from the resulting tokens.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "    If None, no stop words will be used. max_df can be set to a value\n",
       "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
       "    words based on intra corpus document frequency of terms.\n",
       "\n",
       "token_pattern : string\n",
       "    Regular expression denoting what constitutes a \"token\", only used\n",
       "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
       "    or more alphanumeric characters (punctuation is completely ignored\n",
       "    and always treated as a token separator).\n",
       "\n",
       "ngram_range : tuple (min_n, max_n)\n",
       "    The lower and upper boundary of the range of n-values for different\n",
       "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
       "    will be used.\n",
       "\n",
       "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly higher than the given threshold (corpus-specific\n",
       "    stop words).\n",
       "    If float, the parameter represents a proportion of documents, integer\n",
       "    absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "min_df : float in range [0.0, 1.0] or int, default=1\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly lower than the given threshold. This value is also\n",
       "    called cut-off in the literature.\n",
       "    If float, the parameter represents a proportion of documents, integer\n",
       "    absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "max_features : int or None, default=None\n",
       "    If not None, build a vocabulary that only consider the top\n",
       "    max_features ordered by term frequency across the corpus.\n",
       "\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "vocabulary : Mapping or iterable, optional\n",
       "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
       "    indices in the feature matrix, or an iterable over terms. If not\n",
       "    given, a vocabulary is determined from the input documents.\n",
       "\n",
       "binary : boolean, default=False\n",
       "    If True, all non-zero term counts are set to 1. This does not mean\n",
       "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
       "    is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
       "\n",
       "dtype : type, optional\n",
       "    Type of the matrix returned by fit_transform() or transform().\n",
       "\n",
       "norm : 'l1', 'l2' or None, optional\n",
       "    Norm used to normalize term vectors. None for no normalization.\n",
       "\n",
       "use_idf : boolean, default=True\n",
       "    Enable inverse-document-frequency reweighting.\n",
       "\n",
       "smooth_idf : boolean, default=True\n",
       "    Smooth idf weights by adding one to document frequencies, as if an\n",
       "    extra document was seen containing every term in the collection\n",
       "    exactly once. Prevents zero divisions.\n",
       "\n",
       "sublinear_tf : boolean, default=False\n",
       "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "vocabulary_ : dict\n",
       "    A mapping of terms to feature indices.\n",
       "\n",
       "idf_ : array, shape (n_features)\n",
       "    The inverse document frequency (IDF) vector; only defined\n",
       "    if  ``use_idf`` is True.\n",
       "\n",
       "stop_words_ : set\n",
       "    Terms that were ignored because they either:\n",
       "\n",
       "      - occurred in too many documents (`max_df`)\n",
       "      - occurred in too few documents (`min_df`)\n",
       "      - were cut off by feature selection (`max_features`).\n",
       "\n",
       "    This is only available if no vocabulary was given.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
       ">>> corpus = [\n",
       "...     'This is the first document.',\n",
       "...     'This document is the second document.',\n",
       "...     'And this is the third one.',\n",
       "...     'Is this the first document?',\n",
       "... ]\n",
       ">>> vectorizer = TfidfVectorizer()\n",
       ">>> X = vectorizer.fit_transform(corpus)\n",
       ">>> print(vectorizer.get_feature_names())\n",
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
       ">>> print(X.shape)\n",
       "(4, 9)\n",
       "\n",
       "See also\n",
       "--------\n",
       "CountVectorizer\n",
       "    Tokenize the documents and count the occurrences of token and return\n",
       "    them as a sparse matrix\n",
       "\n",
       "TfidfTransformer\n",
       "    Apply Term Frequency Inverse Document Frequency normalization to a\n",
       "    sparse matrix of occurrence counts.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The ``stop_words_`` attribute can get large and increase the model size\n",
       "when pickling. This attribute is provided only for introspection and can\n",
       "be safely removed using delattr or set to None before pickling.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\rschi\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando TfidVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1er Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "Tfidf se lo utiliza cuando tenemos un conjunto de textos. Pero en este caso analizare un texto solamente.\n",
    "\n",
    "Una de las opciones de input es una lista de textos, por eso el formato es `texto = ['bla ... bla']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = [\"Now we have cleaned the data. The next thing we can do is Feature Engineering.\\\n",
    "Feature Engineering is basically a technique for finding Feature or Data from the currently available data.\\\n",
    "There are several ways to do this technique. More often, it is about common sense. Let’s take a look at the \\\n",
    "Embarked data: it is filled with Q, S, or C. The Python library will not be able to process this, since it is\\\n",
    "only able to process numbers. So you need to do something called One Hot Vectorization, changing the column into\\\n",
    "three columns. Let’s say Embarked_Q, Embarked_S, and Embarked_C which are filled with 0 or 1 whether the person\\\n",
    "embarked from that harbor or not. The other example is SibSp and Parch. Maybe there is nothing interesting in both\\\n",
    "of those columns, but you might want to know how big the family was of the passenger who boarded in the ship. You might\\\n",
    "assume that if the family was bigger, then the chance of survival would increase, since they could help each other. On\\\n",
    "other hand, solo people would’ve had it hard. So you want to create another column called family size, which consists\\\n",
    "of sibsp + parch + 1 (the passenger themself). The last example is called bin columns. It is a technique which creates\\\n",
    "ranges of values to group several things together, since you assume it is hard to differentiate things with similar value.\\\n",
    "For example, Age. For a person aged 5 and 6, is there any significant difference? or for person aged 45 and 46, is there\\\n",
    "any big difference?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir vectorizador con parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador = TfidfVectorizer(lowercase=True,  stop_words=\"english\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit-transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz = vectorizador.fit_transform(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una matriz que tiene tantas filas como textos en la lista de inputs (en este caso, solo 1).\n",
    "\n",
    "La cantidad de columnas de esta matriz depende de cuantas palabras distintas hay en todos los inputs juntos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 83)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los titulos de las columas son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['45', '46', 'able', 'age', 'aged', 'assume', 'available', 'basically', 'big', 'bigger', 'bin', 'boarded', 'bothof', 'called', 'chance', 'changing', 'cleaned', 'column', 'columns', 'common', 'consistsof', 'create', 'createsranges', 'currently', 'data', 'difference', 'differentiate', 'embarked', 'embarked_c', 'embarked_q', 'embarked_s', 'engineering', 'example', 'family', 'feature', 'filled', 'finding', 'group', 'hand', 'harbor', 'hard', 'help', 'hot', 'increase', 'interesting', 'intothree', 'isonly', 'know', 'let', 'library', 'look', 'maybe', 'mightassume', 'need', 'numbers', 'onother', 'parch', 'passenger', 'people', 'person', 'personembarked', 'process', 'python', 'say', 'sense', 'ship', 'sibsp', 'significant', 'similar', 'size', 'solo', 'survival', 'technique', 'themself', 'thereany', 'thing', 'things', 'value', 'values', 've', 'vectorization', 'want', 'ways']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizador.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la matriz es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.07179582, 0.07179582, 0.14359163, 0.07179582, 0.14359163,\n",
       "         0.07179582, 0.07179582, 0.07179582, 0.14359163, 0.07179582,\n",
       "         0.07179582, 0.07179582, 0.07179582, 0.21538745, 0.07179582,\n",
       "         0.07179582, 0.07179582, 0.14359163, 0.21538745, 0.07179582,\n",
       "         0.07179582, 0.07179582, 0.07179582, 0.07179582, 0.28718326,\n",
       "         0.14359163, 0.07179582, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.14359163, 0.21538745, 0.21538745, 0.21538745,\n",
       "         0.14359163, 0.07179582, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.14359163, 0.07179582, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.07179582, 0.07179582, 0.14359163, 0.07179582,\n",
       "         0.07179582, 0.07179582, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.14359163, 0.14359163, 0.07179582, 0.14359163,\n",
       "         0.07179582, 0.14359163, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.14359163, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.07179582, 0.21538745, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.14359163, 0.07179582, 0.07179582, 0.07179582,\n",
       "         0.07179582, 0.14359163, 0.07179582]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2o Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "En este caso tenemos un conjunto de 5 textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ['hola, mi nombre es Ricardo', 'Hola, me llamo Juan', 'hola! yo me llamo Julia!', 'Mi nombre es Ana', \"Ana es mi nombre tambien\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "matr = vect.fit_transform(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ana', 'es', 'hola', 'juan', 'julia', 'llamo', 'me', 'mi', 'nombre', 'ricardo', 'tambien', 'yo']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.40065484 0.40065484 0.         0.         0.\n",
      "  0.         0.40065484 0.40065484 0.59824977 0.         0.        ]\n",
      " [0.         0.         0.40382593 0.60298477 0.         0.48648432\n",
      "  0.48648432 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.34582166 0.         0.51637397 0.41660727\n",
      "  0.41660727 0.         0.         0.         0.         0.51637397]\n",
      " [0.57099526 0.47397764 0.         0.         0.         0.\n",
      "  0.         0.47397764 0.47397764 0.         0.         0.        ]\n",
      " [0.46607785 0.38688672 0.         0.         0.         0.\n",
      "  0.         0.38688672 0.38688672 0.         0.57769148 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(matr.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saquemos las stop words del español\n",
    "\n",
    "Para esto tenemos que importar el conjunto de palabras de `nltk.corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "superfluas = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `superfluas` contiene la lista, pero si queremos agregarle la palabra 'tambien' tenemos que usar `.add()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "superfluas.add('tambien')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usa `add` porque es un `set` (array de elementos unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(superfluas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words = superfluas)\n",
    "mat = vec.fit_transform(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ana', 'hola', 'juan', 'julia', 'llamo', 'nombre', 'ricardo']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.48624042 0.         0.         0.         0.48624042\n",
      "  0.72604443]\n",
      " [0.         0.4622077  0.69015927 0.         0.55681615 0.\n",
      "  0.        ]\n",
      " [0.         0.4622077  0.         0.69015927 0.55681615 0.\n",
      "  0.        ]\n",
      " [0.76944707 0.         0.         0.         0.         0.63871058\n",
      "  0.        ]\n",
      " [0.76944707 0.         0.         0.         0.         0.63871058\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(mat.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, hay una columna por palabra del `.get_feature_names()` (y en el mismo orden).\n",
    "\n",
    "Las filas corresponden a los 5 textos del input.\n",
    "\n",
    "Los valores reflejan el peso que se le da a cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
